# -*- coding: utf-8 -*-
"""Jawad_Inclassgradientdescent

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x1rWdbtrSlYtKptUC7nmpz8HqtN7sBcQ
"""

import pandas as pd
import numpy as np

df = pd.read_csv('geyser.csv')
all_data = df.to_numpy()

def compute_cost(data, weights):
    pred = weights[0] + weights[1]*data[:,0]
    cost = 0.5 * np.sum(np.square(pred - data[:,1]))
    return cost

def compute_grad(data, weights):
    #Using the formula we discussed in class for computing the derivative, fill in this function
    durations = data[:, 0]  # Duration values
    intervals = data[:, 1]  # Interval values

    predictions = weights[0] + weights[1] * durations

    errors = predictions - intervals

    dJ_dw0 = np.sum(errors)  # Derivative with respect to w0
    dJ_dw1 = np.sum(errors * durations)  # Derivative with respect to w1

    #Return a numpy array of length 2 with the partial derivatives of the cost function
    return np.array([dJ_dw0, dJ_dw1])

def print_info(i, weights, cost):
    print("Iteration %i"%i)
    print("Weights are currently <%f, %f>"%(w[0], w[1]))
    print("Current cost: %f"%cost)

lr = 0.00001 #learning rate--keep this small
stop = False #becomes true when the cost stops decreasing
i = 1 #count how many iterations in we are
w = np.array([0.0, 0.0]) #initialize to zero
cost_decrease = np.inf
cost = np.inf

while(cost_decrease > 0.001):
    deriv = compute_grad(all_data, w)
    w -= lr*deriv
    newcost = compute_cost(all_data, w)
    cost_decrease = cost - newcost
    cost = newcost
    print_info(i, w, cost)
    i += 1

"""This equation was found after 25 iterations through the data, with the stopping condition being that the cost decrease between iterations was less than 0.001.

To predict the interval for a 3-minute (180 seconds) duration using this equation, we know:

Interval = weights[0] + weights[1] * durations
      = 0.005379 + 0.533253 * 180
      = 95.99 seconds
"""

import pandas as pd
import numpy as np

df = pd.read_csv('geyser.csv')
all_data = df.to_numpy()

def compute_cost(data, weights):
    pred = weights[0] + weights[1]*data[:,0]
    cost = 0.5 * np.sum(np.square(pred - data[:,1]))
    return cost

def compute_grad_stochastic(data_point, weights):
    duration = data_point[0]  # Duration value
    interval = data_point[1]  # Interval value

    prediction = weights[0] + weights[1] * duration

    error = prediction - interval

    dJ_dw0 = error  # Derivative with respect to w0
    dJ_dw1 = error * duration  # Derivative with respect to w1

    return np.array([dJ_dw0, dJ_dw1])

def print_info(i, weights, new_cost):
    print("Iteration %i"%i)
    print("Weights are currently <%f, %f>"%(weights[0], weights[1]))
    print("Current cost: %f"%new_cost)

lr = 0.00001  # Learning rate
i = 1  # Count how many iterations in we are
w = np.array([0.0, 0.0])  # Initialize weights to zero
prev_cost = np.inf
cost = compute_cost(all_data, w)

while True:
    for data_point in all_data:
        deriv = compute_grad_stochastic(data_point, w)
        w -= lr*deriv

    new_cost = compute_cost(all_data, w)
    cost_decrease = prev_cost - new_cost

    if cost_decrease < 0.001:  # Stopping condition
        break

    prev_cost = new_cost
    print_info(i, w, new_cost)
    i += 1

"""This equation was found after 3 iterations through the data, with the stopping condition being that the cost decrease between iterations was less than 0.001.
To predict the interval for a 3-minute (180 seconds) duration using this equation, we know:

Interval = weights[0] + weights[1] * durations
         = 0.004656+0.533281 Ã— 180
         = 95.99 seconds


It makes the same prediction for a 3-minute duration.
"""