# -*- coding: utf-8 -*-
"""Jawad_assignment1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uw0nCjYBQdpZeObXX_dz5DT1CirXnwhW
"""

#I did the code in Google Colab.

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score

train_file = '/content/mnist_train.csv'
test_file = '/content/mnist_test.csv'

def process_file(fname):
    '''assistant function for process_data that opens a CSV, converts it to numpy, and splits it into features and labels'''
    df = pd.read_csv(fname)
    data = df.to_numpy()
    feats = data[:, 1:]  # all columns but the first
    labs = data[:, 0]  # first column only
    return feats, labs

# For atleast 6000 training points and at least 1000 test points
def process_data(train_file, test_file, train_size=6000, test_size=1000):
    '''takes in data CSVs, segments them into features and labels, and converts them into numpy
    optionally also shortens the training set and test set, if the train_size and test_size arguments are specified'''

    if train_size > 60000:
        train_size = 60000
    if test_size > 10000:
        test_size = 10000

    train_feats, train_labs = process_file(train_file)
    test_feats, test_labs = process_file(test_file)

    return train_feats[:train_size], train_labs[:train_size], test_feats[:test_size], test_labs[:test_size]

def euclidean_distance(v1, v2):
    '''calculates the Euclidean distance between vectors v1 and v2'''
    return np.linalg.norm(v1 - v2)

# I took a little bit of help from 'MachineLearningMastery.com' to understand how they implemented it. But I tried to modify the code as the pseudocode in slides.
def knn_predict(train_feats, train_labs, test_feats, k=3):
    predictions = []
    for test_vector in test_feats:
        distances = np.array([euclidean_distance(test_vector, train_vector) for train_vector in train_feats])
        distance_label_pairs = list(zip(train_labs, distances))
        sorted_pairs = sorted(distance_label_pairs, key=lambda pair: pair[1])
        nearest = [pair[0] for pair in sorted_pairs[:k]]
        var = np.bincount(nearest).argmax()
        predictions.append(var)
    return predictions

# Testing for different k values
k_values = [1, 3, 5, 7]
error_rates = {}
train_feats, train_labs, test_feats, test_labs = process_data(train_file, test_file)

for k in k_values:
    predictions = knn_predict(train_feats, train_labs, test_feats, k)
    accuracy = accuracy_score(test_labs, predictions)
    error_rates[k] = 1 - accuracy

# Best k and its confusion matrix
best_k = min(error_rates, key=error_rates.get)
best_predictions = knn_predict(train_feats, train_labs, test_feats, best_k)
conf_matrix = confusion_matrix(test_labs, best_predictions)

print("Error Rates:", error_rates)
print("Best k:", best_k)
print("Confusion Matrix:\n", conf_matrix)