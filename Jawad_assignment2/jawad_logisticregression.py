# -*- coding: utf-8 -*-
"""Jawad_LogisticRegression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXHRKAorIVBHdukJ9K8Z25dc-5QPpuLB
"""

import pandas as pd
import numpy as np
import sklearn.metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score

def read_data(filename):
    #reads in a csv and sements the data
    #randomizes the order of the data, then splits it into different sets
    #returns separate inputs (x) and outputs (y) for each of training, test, and validation
    #also returns a list of column names, which may be useful for determining heavily weighted features
    df = pd.read_csv(filename)
    data = df.to_numpy()
    np.random.shuffle(data)
    test_size = int(data.shape[0]/10)
    data_test = data[:test_size]
    data_val = data[test_size:2*test_size]
    data_train = data[2*test_size:]
    x_train = data_train[:,1:]
    y_train = data_train[:,0]
    x_val = data_val[:,1:]
    y_val = data_val[:,0]
    x_test = data_test[:,1:]
    y_test = data_test[:,0]
    return x_train, y_train, x_val, y_val, x_test, y_test, df.columns.values

def add_ones(x):
    #takes an array of feature vectors and adds a column of 1s to the start
    #useful for logistic regression, since x_0 is always 1
    return np.insert(x, 0, np.ones(x.shape[0]), axis = 1)

def compute_hypothesis(x, weights):
    #computes the hypothesis function for logistic function given data x and weights
    #if x is a single feature vector, will return a scalar
    #if x is a matrix of feature vectors, will return a vector containing the hypothesis for each row
    prod = x.dot(weights)
    return 1/(1 + np.exp(-prod))

def rank_features(weights, feats):
    #takes in a weight vector and an array of feature names
    #returns a sorted array of features, sorted from most negatively weighted to most positively weighted
    #note that feats MUST be a numpy array of the same length as weights
    #if feats[i] does not correspond to weights[i], this will not return accurate results
    imp = np.argsort(weights)
    return feats[imp]

def logistic_regression_sgd(x_train, y_train, learning_rate=0.01, epochs=1000):
    x_train = add_ones(x_train)
    weights = np.zeros(x_train.shape[1])
    for epoch in range(epochs):
        for x, y in zip(x_train, y_train):
            prediction = compute_hypothesis(x, weights)
            err = y - prediction
            weights += learning_rate * err * x
    return weights


def evaluate_performance(x, y, weights):
    x = add_ones(x)
    prediction = compute_hypothesis(x, weights)
    prediction= np.where(prediction >= 0.5, 1, 0)
    accuracy = accuracy_score(y, prediction)
    precision = precision_score(y, prediction)
    recall = recall_score(y, prediction)
    return accuracy, precision, recall

x_train, y_train, x_val, y_val, x_test, y_test, column_names = read_data('mushrooms_logistic.csv')
weights = logistic_regression_sgd(x_train, y_train)

training_accuracy, training_precision, train_recall = evaluate_performance(x_train, y_train, weights)
validation_accuracy, validation_precision, validation_recall = evaluate_performance(x_val, y_val, weights)
test_accuracy, test_precision, test_recall = evaluate_performance(x_test, y_test, weights)

# Printing
print('\n'f"{'Metric':>12} | {'Training':>10} | {'Validation':>10} | {'Test':>10}")
print(f"{'-'*69}")
print(f"{'Accuracy':>12} | {training_accuracy:>10.4f} | {validation_accuracy:>10.4f} | {test_accuracy:>10.4f}")
print(f"{'Precision':>12} | {training_precision:>10.4f} | {validation_precision:>10.4f} | {test_precision:>10.4f}")
print(f"{'Recall':>12} | {train_recall:>10.4f} | {validation_recall:>10.4f} | {test_recall:>10.4f}\n")

features = rank_features(weights[1:], column_names[1:])
print(f"Three features most indicative of a poisonous mushroom:\n {', '.join(features[-3:])}")
print(f"\nThree features most indicative of an edible mushroom:\n {', '.join(features[:3])}")